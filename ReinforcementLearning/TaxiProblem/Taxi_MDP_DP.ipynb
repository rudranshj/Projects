{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Taxi Problem (MDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "geEOnXHeK4oQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oL3_9MAk5cqv"
   },
   "outputs": [],
   "source": [
    "class TaxiEnv:\n",
    "    def __init__(self, states, actions, probabilities, rewards):\n",
    "        self.possible_states = states\n",
    "        self._possible_actions = {st: ac for st, ac in zip(states, actions)}\n",
    "        self._ride_probabilities = {st: pr for st, pr in zip(states, probabilities)}\n",
    "        self._ride_rewards = {st: rw for st, rw in zip(states, rewards)}\n",
    "        self._verify()\n",
    "\n",
    "    def _check_state(self, state):\n",
    "        assert state in self.possible_states, \"State %s is not a valid state\" % state\n",
    "\n",
    "    def _verify(self):\n",
    "        ns = len(self.possible_states)\n",
    "        for state in self.possible_states:\n",
    "            ac = self._possible_actions[state]\n",
    "            na = len(ac)\n",
    "\n",
    "            rp = self._ride_probabilities[state]\n",
    "            assert np.all(rp.shape == (na, ns)), \"invalid Probabilities shape\"\n",
    "        \n",
    "            rr = self._ride_rewards[state]\n",
    "            assert np.all(rr.shape == (na, ns)), \"invalid Rewards shape\"\n",
    "\n",
    "            assert np.allclose(rp.sum(axis=1), 1), \"Probabilities doesn't add up to 1\"\n",
    "\n",
    "    def possible_actions(self, state):\n",
    "        self._check_state(state)\n",
    "        return self._possible_actions[state]\n",
    "\n",
    "    def ride_probabilities(self, state, action):\n",
    "        actions = self.possible_actions(state)\n",
    "        ac_idx = actions.index(action)\n",
    "        return self._ride_probabilities[state][ac_idx]\n",
    "\n",
    "    def ride_rewards(self, state, action):\n",
    "        actions = self.possible_actions(state)\n",
    "        ac_idx = actions.index(action)\n",
    "        return self._ride_rewards[state][ac_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8BwZY8Z5cqw",
    "outputId": "a926f1df-2787-4755-bf32-f98e135ec30d"
   },
   "outputs": [],
   "source": [
    "def make_taxienv():\n",
    "    states = ['A', 'B', 'C']\n",
    "    actions = [['1','2','3'], ['1','2'], ['1','2','3']]\n",
    "    probs = [np.array([[1/2,  1/4,  1/4],\n",
    "                    [1/16, 3/4,  3/16],\n",
    "                    [1/4,  1/8,  5/8]]),\n",
    "\n",
    "            np.array([[1/2,   0,     1/2],\n",
    "                    [1/16,  7/8,  1/16]]),\n",
    "\n",
    "            np.array([[1/4,  1/4,  1/2],\n",
    "                    [1/8,  3/4,  1/8],\n",
    "                    [3/4,  1/16, 3/16]]),]\n",
    "    rewards = [np.array([[10,  4,  8],\n",
    "                        [ 8,  2,  4],\n",
    "                        [ 4,  6,  4]]),   \n",
    "            np.array([[14,  0, 18],\n",
    "                        [ 8, 16,  8]]),    \n",
    "            np.array([[10,  2,  8],\n",
    "                        [6,   4,  2],\n",
    "                        [4,   0,  8]]),]\n",
    "    env = TaxiEnv(states, actions, probs, rewards)\n",
    "    return env\n",
    "env1=make_taxienv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh6g_1u05cqx"
   },
   "source": [
    "# DP Algorithm implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IMYcbfVq5cqx"
   },
   "outputs": [],
   "source": [
    "def dp_solve(taxienv):\n",
    "    states = taxienv.possible_states\n",
    "    values = {s: 0 for s in states}\n",
    "    policy = {s: '0' for s in states}\n",
    "    all_values = [] \n",
    "    all_policies = []\n",
    "    N=10\n",
    "    S=len(states)\n",
    "    J=np.array([[0.0]*S for _ in range(N+1)]) #J[i][a]\n",
    "    \n",
    "    for i in range(N-1,-1,-1):\n",
    "        values = {state: 0 for state in states}\n",
    "        policy = {state: '0' for state in states}\n",
    "        for s in range(S):\n",
    "            acts=taxienv.possible_actions(states[s]) \n",
    "            maxVal,bestAct = -float('inf'),''\n",
    "            for act in acts:\n",
    "                probs=taxienv.ride_probabilities(states[s], act)\n",
    "                rewards=taxienv.ride_rewards(states[s], act)\n",
    "                val=0\n",
    "                for j in range(S):\n",
    "                    val+=probs[j]*(rewards[j]+J[i+1][j])\n",
    "                if val>=maxVal:\n",
    "                    maxVal=val\n",
    "                    bestAct=act\n",
    "            J[i][s]=maxVal\n",
    "            values[states[s]]=maxVal\n",
    "            policy[states[s]]=bestAct\n",
    "        all_values.append(values)\n",
    "        all_policies.append(policy)\n",
    "\n",
    "    results = {\"Expected Reward\": all_values, \"Polcies\": all_policies}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5Ct5_WU1meeo"
   },
   "outputs": [],
   "source": [
    "results=dp_solve(env1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fswnLXrXL2wh",
    "outputId": "97ff00b7-8f81-4b1e-8800-7a68e7578fd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Reward:\n",
      "10 {'A': 8.0, 'B': 16.0, 'C': 7.0}\n",
      "9 {'A': 17.75, 'B': 29.9375, 'C': 17.875}\n",
      "8 {'A': 29.6640625, 'B': 43.421875, 'C': 30.90625}\n",
      "7 {'A': 42.96533203125, 'B': 56.77978515625, 'C': 44.1376953125}\n",
      "6 {'A': 56.295989990234375, 'B': 70.12625122070312, 'C': 57.47271728515625}\n",
      "5 {'A': 69.63932228088379, 'B': 83.47101402282715, 'C': 70.81577682495117}\n",
      "4 {'A': 82.98367631435394, 'B': 96.81558096408844, 'C': 84.16014790534973}\n",
      "3 {'A': 96.32819322496653, 'B': 110.16012235730886, 'C': 97.50466375052929}\n",
      "2 {'A': 109.6727282977663, 'B': 123.50466062361374, 'C': 110.84919888991863}\n",
      "1 {'A': 123.01726577818044, 'B': 136.84919849489233, 'C': 124.19373636617092}\n",
      "\n",
      "Policies:\n",
      "10 {'A': '1', 'B': '1', 'C': '1'}\n",
      "9 {'A': '1', 'B': '2', 'C': '2'}\n",
      "8 {'A': '2', 'B': '2', 'C': '2'}\n",
      "7 {'A': '2', 'B': '2', 'C': '2'}\n",
      "6 {'A': '2', 'B': '2', 'C': '2'}\n",
      "5 {'A': '2', 'B': '2', 'C': '2'}\n",
      "4 {'A': '2', 'B': '2', 'C': '2'}\n",
      "3 {'A': '2', 'B': '2', 'C': '2'}\n",
      "2 {'A': '2', 'B': '2', 'C': '2'}\n",
      "1 {'A': '2', 'B': '2', 'C': '2'}\n"
     ]
    }
   ],
   "source": [
    "print('Expected Reward:')\n",
    "for i in range(10):\n",
    "    print(10-i,results['Expected Reward'][i])\n",
    "\n",
    "print('\\nPolicies:')\n",
    "for i in range(10):\n",
    "    print(10-i,results['Polcies'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AblFN9zNIjwV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of IITM_RL_DP_ASSIGNMENT_v1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
